{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z1Kw47US8g7r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (4.5.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (4.56.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: requests in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: click in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (0.10.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
      "Collecting pyarrow!=4.0.0,>=1.0.0\n",
      "  Downloading pyarrow-4.0.1-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from datasets) (1.19.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from datasets) (2.24.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading huggingface_hub-0.0.14-py3-none-any.whl (43 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "Collecting fsspec>=2021.05.0\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from datasets) (1.0.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from datasets) (20.4)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp38-cp38-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: tqdm>=4.42 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from datasets) (4.56.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.25.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from pandas->datasets) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from packaging->datasets) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\wangshu202040\\anaconda3\\lib\\site-packages (from packaging->datasets) (2.4.7)\n",
      "Installing collected packages: pyarrow, dill, huggingface-hub, multiprocess, fsspec, xxhash, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.7.4\n",
      "    Uninstalling fsspec-0.7.4:\n",
      "      Successfully uninstalled fsspec-0.7.4\n",
      "Successfully installed datasets-1.10.2 dill-0.3.4 fsspec-2021.7.0 huggingface-hub-0.0.14 multiprocess-0.70.12.2 pyarrow-4.0.1 xxhash-2.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: huggingface-hub 0.0.14 has requirement packaging>=20.9, but you'll have packaging 20.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tokenizers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tKPNKCrE-eYt"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing tokenizers: 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-32eed02f08c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBasicTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   2485\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__version__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2486\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2487\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2489\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_LazyModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_import_structure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1697\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1699\u001b[1;33m             \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1700\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m_get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   2479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2481\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2483\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# limitations under the License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m from . import (\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0malbert\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mauto\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\layoutlm\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_BaseLazyModule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mconfiguration_layoutlm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLayoutLMConfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtokenization_layoutlm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLayoutLMTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\layoutlm\\tokenization_layoutlm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenization_bert\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mtokenization_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPreTrainedTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m...\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPaddingStrategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_end_docstrings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m from .tokenization_utils_base import (\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mENCODE_KWARGS_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAddedToken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEncoding\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mEncodingFast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tokenizers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m from .tokenizers import (\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mTokenizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mEncoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing tokenizers: 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "from transformers import BasicTokenizer, GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOHJWRV2-8SF"
   },
   "outputs": [],
   "source": [
    "?BasicTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1622296132757,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "9MPOPjtsAmLs",
    "outputId": "5ce5752d-42ae-418c-b4af-3a0b89971404"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Самое',\n",
       " 'обычное',\n",
       " 'предложение',\n",
       " 'для',\n",
       " 'обработки',\n",
       " ',',\n",
       " 'неправда',\n",
       " 'ли',\n",
       " '?']"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic = BasicTokenizer()\n",
    "basic.do_lower_case = False\n",
    "basic.tokenize('Самое обычное предложение для обработки, неправда ли?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lc-4mVHWGDxD"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1622296171846,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "TUUPbZ0gerfm",
    "outputId": "53bc48ea-1452-4c32-8c67-1da605dcae6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [15496, 995], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1622296182976,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "uafqweW5GHag",
    "outputId": "f02cb082-a05e-406a-ae8a-84e5aa73e31d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 995]"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello world\")['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1622296191572,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "na7E1YDZGbVy",
    "outputId": "6ca005dc-b5b4-4a32-d130-1eb0e4f21cf9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496]"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjycyrtVGkOV"
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = 'PAD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi-pM4QOHvgD"
   },
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzPZa6pFSI9B"
   },
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1622296233608,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "IKIR_Rpd--kN",
    "outputId": "64a6336f-b1d7-4160-c1e3-89a0ad7e0d70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    }
   ],
   "source": [
    "text_dataset = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdBxEjbNHxX3"
   },
   "outputs": [],
   "source": [
    "from tokenizers import normalizers, pre_tokenizers, Tokenizer, trainers, CharBPETokenizer\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xFri39nOL8D"
   },
   "source": [
    "## train form dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60yagAvGIJkD"
   },
   "outputs": [],
   "source": [
    "bpe_tokenizer = Tokenizer(BPE())\n",
    "bpe_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "bpe_tokenizer.normalizer = normalizers.Lowercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_QVRjjpIaTh"
   },
   "outputs": [],
   "source": [
    "# Build an iterator over this dataset\n",
    "def batch_iterator():\n",
    "    batch_length = 1000\n",
    "    for i in range(0, len(text_dataset[\"train\"]), batch_length):\n",
    "        yield text_dataset[\"train\"][i : i + batch_length][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNJp29nOIey-"
   },
   "outputs": [],
   "source": [
    "bpe_tokenizer.train_from_iterator(batch_iterator(), length=len(text_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 379,
     "status": "ok",
     "timestamp": 1622296302775,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "E2OnZt_nOh4Z",
    "outputId": "3cd38a31-65a1-45d4-9525-e5b9b0c622f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=172, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 81,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_tokenizer.encode(text_dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGV5XBMsWXou"
   },
   "outputs": [],
   "source": [
    "bpe_tokenizer.encode(text_dataset['train'][0]['text']).ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1622296317234,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "R_x93JmlKEtK",
    "outputId": "27fe2bef-9894-4c12-c282-f18e5c32c51a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1622296320399,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "Xrx4NX66Ip5n",
    "outputId": "4594f425-e77a-4d0f-f703-e2511ab5bbd8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'brom well high is a cartoon comedy . it ran at the same time as some other programs about school life , such as \" teachers \". my 35 years in the teaching profession lead me to believe that brom well high \\' s satire is much closer to reality than is \" teachers \". the sc ramble to survive financially , the insightful students who can see right through their pathetic teachers \\' pomp , the pett iness of the whole situation , all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school , i immediately recalled ......... at .......... high . a classic line : inspector : i \\' m here to sack one of your teachers . student : welcome to brom well high . i expect that many adults of my age think that brom well high is far fetched . what a pity that it isn \\' t !'"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(bpe_tokenizer.encode(text_dataset['train'][0]['text']).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw8kRNIpJukq"
   },
   "outputs": [],
   "source": [
    "bpe_tokenizer.save('vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-I2LVGsRKlXz"
   },
   "outputs": [],
   "source": [
    "new_tokenizer = Tokenizer(BPE())\n",
    "new_tokenizer = new_tokenizer.from_file('vocab.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1622296392914,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "Xu2oq0kjLRLr",
    "outputId": "2ef25da9-4576-4a7e-c44a-5c69d1a75ab2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'brom well high is a cartoon comedy . it ran at the same time as some other programs about school life , such as \" teachers \". my 35 years in the teaching profession lead me to believe that brom well high \\' s satire is much closer to reality than is \" teachers \". the sc ramble to survive financially , the insightful students who can see right through their pathetic teachers \\' pomp , the pett iness of the whole situation , all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school , i immediately recalled ......... at .......... high . a classic line : inspector : i \\' m here to sack one of your teachers . student : welcome to brom well high . i expect that many adults of my age think that brom well high is far fetched . what a pity that it isn \\' t !'"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(new_tokenizer.encode(text_dataset['train'][0]['text']).tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "209Y0qWuOFXK"
   },
   "source": [
    "## train from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTS4eg64OvDW"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1634,
     "status": "ok",
     "timestamp": 1622296423821,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "BUNmeVIRN8X2",
    "outputId": "e0c95cab-8ae4-431f-b1fe-2f3ee5a8c895"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:01<00:00, 15657.24it/s]\n"
     ]
    }
   ],
   "source": [
    "to_file = []\n",
    "\n",
    "for text in tqdm(text_dataset['train']):\n",
    "  to_file.append(text['text'])\n",
    "\n",
    "with open('train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(to_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB2RnVupP1Td"
   },
   "outputs": [],
   "source": [
    "!head train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7laPdiTgPDCr"
   },
   "outputs": [],
   "source": [
    "tokenizer = CharBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OD0UltuzPSuq"
   },
   "outputs": [],
   "source": [
    "tokenizer.train([ \"./train.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1622296504214,
     "user": {
      "displayName": "Никита Сидоров",
      "photoUrl": "",
      "userId": "18305587839672425865"
     },
     "user_tz": -180
    },
    "id": "lOqVXANfPZkQ",
    "outputId": "c6cfc317-c166-45c0-9ac4-58ef40264209"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Brom well</w> High</w> is</w> a</w> cartoon</w> comedy</w> .</w> It</w> ran</w> at</w> the</w> same</w> time</w> as</w> some</w> other</w> programs</w> about</w> school</w> life</w> ,</w> such</w> as</w> \"</w> Te ach ers</w> \"</w> .</w> My</w> 35</w> years</w> in</w> the</w> teaching</w> profession</w> lead</w> me</w> to</w> believe</w> that</w> Brom well</w> High</w> \\'</w> s</w> satire</w> is</w> much</w> closer</w> to</w> reality</w> than</w> is</w> \"</w> Te ach ers</w> \"</w> .</w> The</w> sc ramble</w> to</w> survive</w> financially</w> ,</w> the</w> insightful</w> students</w> who</w> can</w> see</w> right</w> through</w> their</w> pathetic</w> teachers</w> \\'</w> pom p</w> ,</w> the</w> pe tt iness</w> of</w> the</w> whole</w> situation</w> ,</w> all</w> remind</w> me</w> of</w> the</w> schools</w> I</w> knew</w> and</w> their</w> students</w> .</w> When</w> I</w> saw</w> the</w> episode</w> in</w> which</w> a</w> student</w> repeatedly</w> tried</w> to</w> burn</w> down</w> the</w> school</w> ,</w> I</w> immediately</w> recalled</w> .</w> .</w> .</w> .</w> .</w> .</w> .</w> .</w> .</w> at</w> .</w> .</w> .</w> .</w> .</w> .</w> .</w> .</w> .</w> .</w> High</w> .</w> A</w> classic</w> line</w> :</w> IN SPE CTOR</w> :</w> I</w> \\'</w> m</w> here</w> to</w> sack</w> one</w> of</w> your</w> teachers</w> .</w> STU D ENT</w> :</w> Welcome</w> to</w> Brom well</w> High</w> .</w> I</w> expect</w> that</w> many</w> adults</w> of</w> my</w> age</w> think</w> that</w> Brom well</w> High</w> is</w> far</w> fetched</w> .</w> What</w> a</w> pity</w> that</w> it</w> isn</w> \\'</w> t</w> !</w>'"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(text_dataset['train'][0]['text']).tokens\n",
    "' '.join(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3vJUWb2Z-Xv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5xFri39nOL8D",
    "209Y0qWuOFXK"
   ],
   "name": "transformers_sreencast.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
